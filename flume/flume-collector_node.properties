############################################
#  producer config
###########################################

#agent section
producer.sources = s
producer.channels = hdfsChannel

producer.sinks = hdfsSink

#source section
producer.sources.s.type = avro
producer.sources.s.bind = 0.0.0.0
producer.sources.s.port = 50001
#interceptor
#producer.sources.s.interceptors = it ih
#producer.sources.s.interceptors.it.type = timestamp
#producer.sources.s.interceptors.ih.type = host
producer.sources.s.interceptors = i2
producer.sources.s.interceptors.i2.type=org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
producer.sources.s.interceptors.i2.headerName=key
producer.sources.s.interceptors.i2.preserveExisting=false

producer.sources.s.channels = hdfsChannel
#---------hdfsSink ------------------
#producer.sinks.hdfsSink.type = hdfs
#producer.sinks.hdfsSink.hdfs.path = hdfs:/user/hadoop/applog/%{appEnv}/%{appId}/%Y%m
#producer.sinks.hdfsSink.hdfs.round = true
#producer.sinks.hdfsSink.hdfs.filePrefix= applog_%{appId}_%d
##如果600s内没有新数据,则滚动生成新文件
#producer.sinks.hdfsSink.hdfs.idleTimeout = 600
##按照时间间隔15分钟滚动生成新文件
#producer.sinks.hdfsSink.hdfs.rollInterval = 900
##128M，当文件大小达到128M后，滚动生成新文件
#producer.sinks.hdfsSink.hdfs.rollSize = 128000000
##不按照event条数滚动生成新文件
#producer.sinks.hdfsSink.hdfs.rollCount=0
#producer.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
#producer.sinks.hdfsSink.hdfs.batchSize = 100
#producer.sinks.hdfsSink.hdfs.writeFormat = Text
#producer.sinks.hdfsSink.hdfs.fileType = DataStream
#producer.sinks.hdfsSink.hdfs.callTimeout=30000
#producer.sinks.hdfsSink.hdfs.minBlockReplicas=1

#producer.sinks.hdfsSink.channel = hdfsChannel
# Each channel's type is defined.
producer.channels.hdfsChannel.type = memory
producer.channels.hdfsChannel.capacity = 1000000
producer.channels.hdfsChannel.transactionCapacity=1000000

############################################
#   consumer config
###########################################

consumer.sources = s
consumer.channels = c
consumer.sinks = r

#consumer.sources.s.type = seq
consumer.sources.s.channels = c
consumer.sinks.r.type = logger

consumer.sinks.r.channel = c
consumer.channels.c.type = memory
consumer.channels.c.capacity = 100

consumer.sources.s.type = org.apache.flume.plugins.KafkaSource
consumer.sources.s.zookeeper.connect=10.9.173.90:2181
consumer.sources.s.group.id=testGroup
consumer.sources.s.zookeeper.session.timeout.ms=400
consumer.sources.s.zookeeper.sync.time.ms=200
consumer.sources.s.auto.commit.interval.ms=1000
consumer.sources.s.custom.topic.name=applog
consumer.sources.s.custom.thread.per.consumer=4
